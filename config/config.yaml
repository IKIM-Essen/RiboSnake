# This file should contain everything to configure the workflow on a global scale.
# In case of sample based data, it should be complemented by a samples.tsv file that contains
# one row per sample. It can be parsed easily via pandas.
pepfile: config/pep/config.yaml

# If testing set true
testing: False
# Modus: If true, analysis will be done with DADA2 instead of vsearch
DADA2: False
# Shorter analysis for small number of samples or samples without metadata
reduced-analysis: True
# Whether to use bowtie2 for excluding human host contamination or not
bowtie: False
# Set if environmental or human data is processed
data-type: human
# Paths to the directories holding the fastqs
input: ../incoming/
data: data/
output: ../report/
# Path to the sheet holding metadata information
metadata: config/pep/metadata.txt
# Remove columns from analysis
remove-columns: ['subject']
# Paths to the databases used for classification and taxonomy
database:
  Silva: True 
  download-path-seq: https://data.qiime2.org/2022.2/common/silva-138-99-seqs.qza
  download-path-tax: https://data.qiime2.org/2022.2/common/silva-138-99-tax.qza
# Forward and reverse adapters used for sequencing
adapter1: TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG
adapter2: GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG
# Type of data, single-end: 'SampleData[SequencesWithQuality]' or paired-end: 'SampleData[PairedEndSequencesWithQuality]'
datatype: 'SampleData[PairedEndSequencesWithQuality]'
#Primertrimming parameters
primertrimming:
  forward: CCTACGGGNGGCWGCAG
  reverse: GACTACHVGGGTATCTAATCC
  # Maximum allowed error rate
  error_rate: 0.1
  # Remove multiple occurrences of an adapter if it is repeated, up to `times` times
  rep_times: 2
  # Require at least `overlap` bases of overlap between read and adapter for an adapter to be found
  overlap: 5
  # Discard reads shorter than specified value
  min_length: 8
sequence_joining:
  # Minimum overlap length of forward and reverse reads for joining.
  seq_join_length: 30
  # Sequences shorter than minlen after truncation are discarded. 
  minlen: 1
  # Maximum number of mismatches in the forward/reverse read overlap for joining. Increase it, if you want reads to be merged that are small/won't match in the first place.
  maxdiffs: 10
  # Number of threads to use
  threads: 4
# Threads to be used by the different methods
threads: 15
# Parameters necessary for filtering
filtering:
  # Relative threshold for the abundance filtering
  relative-abundance-filter: 0.001
  # Minimum length of a sequence to be retained
  min-seq-length: 200
  # Minimum quality score for sequences to pass filter
  phred-score: 20
  # Maximum number of undetermined base calls ("N")
  max-ambiguity: 50
  # The minimum length that a sequence read can be following truncation and still be retained
  min-length-frac: 0.75
  # Increasing this value tends to reduce the number of false positives and to decrease sensitivity.
  chimera-minh: 0.35
  # Percentual identity between query sequence and reference human genome
  perc-identity: 0.93
  # Percent of query sequence that must align to reference in order to be accepted as a hit
  perc-query-aligned: 0.93
# Parameters for plots
metadata-parameters:
  taxa-heatmap-column: run_date
  beta-metadata-column: run_date
  gneiss-metadata-column: run_date
  absolute-taxa-name: subject
  # Specify which axes to cluster: ('features', 'samples', 'both', 'none')
  cluster: features
# Parameters for classification
classification:
  # Reject match if percent identity to query is lower
  perc-identity: 0.97
  # Range (1,None), String("all"), Maximum number of hits to keep for each query
  maxaccepts: 1
  # Range(1,None), String("all"), Maximum number of non-matching target sequences to consider before stopping the search
  maxrejects: 1
  # Reject match if query alignment coverage per high-scoring pair is lower. 
  query-cov: 0.8
  # Minimum fraction of assignments must match top hit to be accepted as consensus assignment.
  min-consensus: 0.51
# Parameters for de-novo clustering with vsearch
clustering:
  #The percent identity at which clustering should be performed
  perc-identity: 0.99
# Parameters set for alpha- and beta-rarefaction
rarefaction:
  # The maximum rarefaction depth. Must be greater than min-depth
  max-depth: 500 #400
  # The total frequency that each sample should be rarefied to prior to computing the diversity metric
  sampling_depth: 100
  # How often rarefaction should be repeated
  repeats: 50
  # The beta diversity metric to be computed
  metric: euclidean
  # Samples can be clustered with neighbor joining or UPGMA ("nj", "upgma")
  clustering_method: nj
# Parameters for diversity analysis
diversity:
  # Parameters for alpha-diversity analysis
  alpha:
    # Metric to be computed
    diversity-metric: ["pielou_e"]
    # Metric to be computed
    phylogeny-metric: ["faith_pd"]
    # Alpha-correlation method: spearman or pearson
    correlation-method: 'spearman'
  # Parameters for beta-diversity analysis
  beta:
    # Metric to be computed
    diversity-metric: ["euclidean"]
    # A pseudocount to handle zeros for compositional metrics
    diversity-pseudocount: 1
    # The number of concurrent jobs to use in performing this calculation
    diversity-n-jobs: 3
    # Correlation test to be applied ('spearman', 'pearson')
    correlation-method: 'spearman'
    # The number of permutations to be run when computing p-values
    correlation-permutations: 999
    # The beta diversity metric to be computed ('weighted_normalized_unifrac',
    # 'generalized_unifrac', 'weighted_unifrac', 'unweighted_unifrac')
    phylogeny-metric: ['weighted_normalized_unifrac']
    # Perform variance adjustment based on Chang et al. BMC Bioinformatics 2011
    phylogeny-variance-adjusted: True
# If DADA2 is set to true, these parameters are used for denoising and clustering paired-end-data with DADA2
dada2-paired:
  # Position at which forward reads should be truncated at the 3' end due to low quality
  trunc-len-f: 240
  # Position at which reverse reads should be truncated at the 3' end due to low quality
  trunc-len-r: 240
  # Position at which forward reads should be truncated at the 5' end due to low quality
  trim-left-f: 10
  # Position at which reverse reads should be truncated at the 5' end due to low quality
  trim-left-r: 10
  # Forward reads with number of expected errors higher than this value will be discarded
  max-ee-f: 2.0
  # Reverse reads with number of expected errors higher than this value will be discarded.
  max-ee-r: 2.0
  # Reads are truncated at the first instance of a quality score less than or equal to this value
  trunc-q: 5
  # The minimum length of the overlap required for merging the forward and reverse reads
  min-overlap: 12
  # The method used to pool samples for denoising ('independent', 'pseudo')
  pooling-method: 'independent'
  # The method used to remove chimeras ('none', 'consensus', 'pooled')
  chimera-method: 'consensus'
  # The minimum abundance of potential parents of a sequence being tested as chimeric, expressed as a 
  # fold-change versus the abundance of the sequence being tested
  min-fold-parent-over-abundance: 1.0
  # The number of reads to use when training the errormodel
  n-reads-learn: 1000000
# If DADA2 is set to true, these parameters are used for denoising and clustering single-end-data with DADA2
dada2-single:
  # Position at which reads should be truncated at the 3' end due to low quality
  trunc-len: 240
  # Position at which reads should be truncated at the 5' end due to low quality
  trim-left: 10
  # Reads with number of expected errors higher than this value will be discarded
  max-ee: 2.0
  # Reads are truncated at the first instance of a quality score less than or equal to this value
  trunc-q: 5
  # The method used to pool samples for denoising ('independent', 'pseudo')
  pooling-method: 'independent'
  # The method used to remove chimeras ('none', 'consensus', 'pooled')
  chimera-method: 'consensus'
  # The minimum abundance of potential parents of a sequence being tested as chimeric, expressed as a 
  # fold-change versus the abundance of the sequence being tested
  min-fold-parent-over-abundance: 1.0
  # The number of reads to use when training the errormodel
  n-reads-learn: 1000000
# Feature classification with ancom